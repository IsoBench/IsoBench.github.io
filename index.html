<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations">
  <meta name="keywords" content="Large Lanaguge Models, Multimodal Foundation Models, GPT-4, Claude-3, Gemini">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations</title>

<!-- Google Tag Manager -->
<!-- <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script> -->
<!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/tifa.png"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>
  <!-- Google Tag Manager (noscript) -->
  <!-- <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript> -->
  <!-- End Google Tag Manager (noscript) -->

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/abs/2305.00586">
            How does GPT-2 compute greater-than?
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2310.17086">
            How Transformers learn in-context?
          </a>
          <a class="navbar-item" href="https://DeLLMa.github.io">
            DeLLMa: Decision Making under Uncertainty with LLMs
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2310.07972">
            How to interpret diffusion models?
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2311.17946">
            How to improve text-to-image alignment?
          </a>
          <a class="navbar-item" href="https://arxiv.org/abs/2312.06550">
            LLM360: Towards Fully Transparent Open-Source LLMs
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><span style="color: rgb(153, 27, 30);"><b>IsoBench</b></span>: Benchmarking Multimodal Foundation Models on Isomorphic Representations</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sites.google.com/usc.edu/deqingfu">Deqing Fu</a><sup>&#120595;*</sup>,
            </span>
            <span class="author-block">
              <a href="https://ruohaog.netlify.app/">Ruohao Guo</a><sup>&#120575;*</sup>,
            </span>
            <span class="author-block">
              <a href="https://ghazalkhalighinejad.github.io">Ghazal Khalighinejad</a><sup>&#120582;*</sup>,
            </span>
            <span class="author-block">
              <a href="https://ollieliu.com/">Ollie Liu</a><sup>&#120595;*</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://users.cs.duke.edu/~bdhingra/">Bhuwan Dhingra</a><sup>&#120582;</sup>,
            </span>
            <span class="author-block">
              <a href="https://dyogatama.github.io/">Dani Yogatama</a><sup>&#120595;</sup>,
            </span>
            <span class="author-block">
              <a href="https://robinjia.github.io/">Robin Jia</a><sup>&#120595;</sup>,
            </span>
            <span class="author-block">
              <a href="https://willieneis.github.io/">Willie Neiswanger</a><sup>&#120595;</sup>
            </span>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>&#120595; </sup>University of Southern California</span>&nbsp;&nbsp;
            <span class="author-block"><sup>&#120575; </sup>Georgia Institute of Technology</span>&nbsp;&nbsp;
            <span class="author-block"><sup>&#120582; </sup>Duke University</span>
            <br>
            <span style="font-size:small"><sup>* </sup><i>Equal Contribution. Ordered Alphabetically</i></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2404.01266"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Data Link. -->
              <span class="link-block">
                <span class="button-wrapper">
                  <a href="https://huggingface.co/datasets/isobench/IsoBench" 
                     class="external-link button is-normal is-rounded is-dark">
                    <i style="font-size: small" class="fa fa-database"></i> 
                    <span>&nbsp;Data</span>
                  </a>
                </span>
              </span>

              <span class="link-block">
                <span class="button-wrapper">
                  <a class="external-link button is-normal is-rounded is-dark" href="https://github.com/IsoBench/IsoBench-Eval">
                    <i style="font-size: small" class="fa fa-database"></i> 
                    <span>&nbsp;Evaluation Suite</span>
                  </a>
                </span>
              </span>
              
              
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div style="text-align: center;"><br/>
        <img src="./static/images/isobench-teaser.png" alt="isobench teaser"
        width="device-width" class="center">
      </div>
      <h2 class="subtitle has-text-centered" style="max-width:870px;">
        Do multimodal foundation models treat every modality equally?
        <br>
        Introducing <span style="color: rgb(153, 27, 30);"><b>IsoBench</b></span>, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple <b>isomorphic representations</b> of inputs, such as visual, textual, and mathematical presentations. 
        <br>
        Click <a href="#isobench-full">here</a> to see full IsoBench results.
      </h2>
      <div style="text-align: center;"><br/>
        <img src="./static/images/tasks.png" alt="isobench tasks"
        width="device-width" class="center">
      </div>
    </div>
  </div>


  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="column content has-text-justified">
          <p>
            Current foundation models exhibit impressive capabilities when prompted either with text only or with both image and text inputs. But do their capabilities change depending on the input modality? In this work, we propose <span style="color: rgb(153, 27, 30);"><b>IsoBench</b></span>, a benchmark dataset containing problems from four major areas: math, science, algorithms, and games. Each example is presented with multiple <b>isomorphic representations</b> of inputs, such as visual, textual, and mathematical presentations. IsoBench provides fine-grained feedback to diagnose performance gaps caused by the form of the representation. Across various foundation models, we observe that on the same problem, models have a consistent preference towards textual representations. Most prominently, when evaluated on all IsoBench problems, Claude-3 Opus performs 28.7 points worse when provided with images instead of text; similarly, GPT-4 Turbo is 18.7 points worse and Gemini Pro is 14.9 points worse. Finally, we present two prompting techniques, <i>IsoCombination</i> and <i>IsoScratchPad</i>, which improve model performance by considering combina- tions of, and translations between, different input representations.
          </p>
          
        </div>
      </div>
    </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <div class="column is-four-fifths">


          <h3 class="title is-3 has-text-centered">IsoBench Results</h3>


          <img src="./static/images/isobench-results.png" alt="isobench results" width="device-width">

          <p>
            Within each subject, the best scores in processing image representations are highlighted in <span style="color: rgb(66, 133, 244);"><b>blue</b></span>, and the best in text representations are in <span style="color: rgb(219, 68, 55);"><b>red</b></span>. Overall, GPT-4 Turbo is the best for images and Claude-3 Opus is the best for text. Across four subjects within IsoBench, multimodal foundation models have a strong preference for text modalities. The gap in accuracy between the best text representation and its isomorphic image representation can be as large as <b>28.7%</b>.
          </p>


      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <div class="column is-four-fifths">


        <h3 class="title is-3 has-text-centered">IsoCombination and IsoScratchPad</h3>


        <img src="./static/images/iso-cb-sp.png" alt="isoCB and isoSP" width="device-width">

        <p>
        Illustration of <i>IsoCombination</i> (IsoCB) and <i>IsoScratchPad</i> (IsoSP). IsoCB combines all representations provided by a user and constructs one unified prompt for a foundation model. IsoSP is a two-step prompting method, where a foundation model first describes an image and then uses the textual description as the sole representation for a given task.
        </p>

        <img src="./static/images/iso-cb-sp-results.png" alt="isoCB and isoSP Results" width="device-width">
        <p>
          Best prompting methods are highlighted in <span style="color:rgb(219, 68, 55);"><b>red</b></span> and improvements over image-only prompts are in <span style="color:rgb(15, 157, 88);">(green)</span>. Both methods improve performance in comparison with image representations, and for certain domains, IsoCombination additionally improves performance relative to text representations.
        </p>

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <div class="column is-four-fifths">

        <a id="isobench-full"> 
        <h2 class="title is-2 has-text-centered">Full IsoBench Results</h3>

        
        <img src="./static/images/plots-all-models.png" alt="IsoBench Full" width="device-width">

        <img src="./static/images/isobench-full-table.png" alt="IsoBench Full Table" width="device-width">
        </a>

        <p>Full IsoBench results including more API-access models (GPT-3.5 Turbo and PaLM-2) and open-source models (LLaMa-2 70B and LLaVA-1.5 13B).</p>
      </div>
    </div>
  </div>
</section>





<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{fu2024isobench,
      title={{I}so{B}ench: Benchmarking Multimodal Foundation Models on Isomorphic Representations}, 
      author={Deqing Fu and Ruohao Guo and Ghazal Khalighinejad and Ollie Liu and Bhuwan Dhingra and Dani Yogatama and Robin Jia and Willie Neiswanger},
      booktitle={First Conference on Language Modeling (COLM)},
      year={2024},
      note={First four authors contributed equally.}
}</code></pre>
  </div>
</section>




<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
